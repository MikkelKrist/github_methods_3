---
title: "practical_exercise_7 , Methods 3, 2021, autumn semester"
author: '[FILL IN YOUR NAME]'
date: "[FILL IN THE DATE]"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>


# Exercises and objectives

1) Estimate bias and variance based on a true underlying function  
2) Fitting training data and applying it to test sets with and without regularization  

For each question and sub-question, please indicate one of the three following answers:  
    i. I understood what was required of me  
    ii. I understood what was required of me, but I did not know how to fulfil the requirement  
    iii. I did not understand what was required of me  

# EXERCISE 1 - Estimate bias and variance based on a true underlying function 
```{r}
library(reticulate)
```


We can express regression as $y = f(x) + \epsilon$ with $E[\epsilon] = 0$ and $var(\epsilon) = \sigma^2$ ($E$ means expected value)  
  
For a given point: $x_0$, we can decompose the expected prediction error , $E[(y_0 - \hat{f}(x_0))^2]$ into three parts - __bias__, __variance__ and __irreducible error__ (the first two together are the __reducible error__):

The expected prediction error is, which we also call the __Mean Squared Error__:  
$E[(y_0 - \hat{f}(x_0))^2] =  bias(\hat{f}(x_0))^2 + var(\hat{f}(x_0)) + \sigma^2$
  
where __bias__ is;
  
$bias(\hat{f}(x_0)) = E[\hat{f}(x_0)] - f(x_0)$

1) Create a function, $f(x)$ that squares its input. This is our __true__ function  
    i. generate data, $y$, based on an input range of [0, 6] with a spacing of 0.1. Call this $x$
    ii. add normally distributed noise to $y$ with $\sigma=5$ (set a seed to 7 `np.random.seed(7)`) to $y$ and call it $y_{noisy}$
    iii. plot the true function and the generated points  
    
```{python}

```
    
2) Fit a linear regression using `LinearRegression` from `sklearn.linear_model` based on $y_{noisy}$ and $x$ (see code chunk below associated with Exercise 1.2)  
    i. plot the fitted line (see the `.intercept_` and `.coef_` attributes of the `regressor` object) on top of the plot (from 1.1.iii)
    ii. now run the code chunk below associated with Exercise 1.2.ii - what does X_quadratic amount to?
    iii. do a quadratic and a fifth order fit as well and plot them (on top of the plot from 1.2.i)

```{python}
#Exercise 2

def truefunction(x): #a function that squares its input
    p = x**2
    return(p)


x_2 = np.arange(0, 6, 0.1) #we create input for our function

y_true = truefunction(x_2) #we generate data from our true function (is this our true function?)

num_sampl = len(y_true) #we create a vector of length of our y_true vector

np.random.seed(7) #we set the seed for our random generation of numbers below

y_noise = y_true+np.random.normal(loc=0, scale=5, size=num_sampl) # we complete the data generation with normally distributed noise added (with sigma = 5)


#we now proceed to plot the generated points above and plot the true function on top of those (CHECK AGAIN: IS THIS CORRECT???)

plt.clf() 
plt.scatter(x_2, y_noise)
plt.scatter(x_2, y_true)
#plt.plot(y_noise, 'r')#delete? #perhaps just add a regression line on top of this noise
plt.legend(['generated data (with noise)', 'true function'])
plt.show()



# Exercise 2.2
from sklearn.linear_model import LinearRegression

num_sampl = len(y_true)
X_2
X_2 = np.zeros(shape=(num_sampl, 2))
X_2[:, 0] = x_2 ** 0 
X_2[:, 1] = x_2 ** 1 

regressor = LinearRegression().fit(X_2, y_noise)

y_hat_LG = regressor.intercept_+regressor.coef_[1]*x_2

plt.clf() 
plt.scatter(x_2, y_noise)
plt.scatter(x_2, y_true)
plt.plot(y_hat_LG)
plt.xlim([0, 6])
plt.ylim([-10, 40])
plt.show()


# Exercise 2.2.ii
from sklearn.preprocessing import PolynomialFeatures
quadratic = PolynomialFeatures(degree=2)
X_quadratic = quadratic.fit_transform(x_2.reshape(-1, 1)) #This amounts to mupltiple linear regression of a polynomial equation of degree 2
regressor2 = LinearRegression().fit(X_quadratic, y_noise)

y_hat_MLG = regressor2.intercept_+regressor2.coef_[1]*x_2+regressor2.coef_[2]*x_2**2


#We now do a fifth order polynomial
num_sampl = len(y_noise)
X_5 = np.zeros(shape=(num_sampl, 3))
X_5[:, 0] = x_2 ** 0 
X_5[:, 1] = x_2 ** 1 
X_5[:, 2] = x_2**5

regressor5 = LinearRegression().fit(X_5, y_noise)

y_hat_MLG5 = regressor5.intercept_+regressor5.coef_[1]*x_2+regressor5.coef_[2]*x_2**5


#The plots with xlim and ylim
plt.clf() 
plt.scatter(x_2, y_noise)
plt.scatter(x_2, y_true)
plt.plot(x_2, y_hat_LG)
plt.plot(x_2, y_hat_MLG)
plt.plot(x_2, y_hat_MLG5)
plt.xlim([0, 6])
plt.ylim([-10, 40])
plt.show()

#The plots without the xlim and ylim, and they show how the polynomial fits are fitted to the data, although the original data created above does not reach as far as the polynomial fits, so it looks very off
plt.figure() 
plt.scatter(x_2, y_noise) #generated data
plt.scatter(x_2, y_true) #true function
plt.plot(x_2, y_hat_LG) #Linear reg
plt.plot(x_2, y_hat_MLG) #second order lin reg
plt.plot(x_2, y_hat_MLG5)#Fifth order lin reg
plt.legend(['generated data', 'true function', 'linear reg', 'second order', 'fifth order'])
plt.show()



```
    
3) Simulate 100 samples, each with sample size `len(x)` with $\sigma=5$ normally distributed noise added on top of the true function    
    i. do linear, quadratic and fifth-order fits for each of the 100 samples      
```{python}




```

    
    ii. create a __new__ figure, `plt.figure`, and plot the linear and the quadratic fits (colour them appropriately); highlight the true value for $x_0=3$. From the graphics alone, judge which fit has the highest bias and which has the highest variance for $x_0$  
    iii. create a __new__ figure, `plt.figure`, and plot the quadratic and the fifth-order fits (colour them appropriately); highlight the true value for $x_0=3$. From the graphics alone, judge which fit has the highest bias and which has the highest variance for $x_0$  
    iv. estimate the __bias__ and __variance__ at $x_0$ for the linear, the quadratic and the fifth-order fits (the expected value $E[\hat{f}(x_0)] - f(x_0)$ is found by taking the mean of all the simulated, $\hat{f}(x_0)$, differences)  
    v. show how the __squared bias__ and the __variance__ is related to the complexity of the fitted models  
    vi. simulate __epsilon__: `epsilon = np.random.normal(scale=5, size=100)`. Based on your simulated values of __bias, variance and epsilon__, what is the __Mean Squared Error__ for each of the three fits? Which fit is better according to this measure? 

    
```{python, eval=FALSE}
# Exercise 1.2
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit() ## what goes in here?
```    

```{python, eval=FALSE}
# Exercise 1.2.ii
from sklearn.preprocessing import PolynomialFeatures
quadratic = PolynomialFeatures(degree=2)
X_quadratic = quadratic.fit_transform(x.reshape(-1, 1))
regressor = LinearRegression()
regressor.fit() # what goes in here?
y_quadratic_hat # calculate this

```


---
title: "practical_exercise_6, Methods 3, 2021, autumn semester"
author: '[FILL IN YOUR NAME]'
date: "[FILL IN THE DATE]"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>


# Exercises and objectives

1) Get acquainted with _Python_, and learn some of the differences between it and _R_  
2) Estimate bias and variance based on a true underlying function  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: All exercises should be done in _Python_

# EXERCISE 1 Get acquainted with _Python_ learn some of the differences between it and _R_  

To make sure that _Python_ runs within _R Markdown_, make sure you have the _reticulate_ package installed `install.packages('reticulate')`  
Also create a text file that is called _.Renviron_ (remember the dot) placed in the folder where your _RProj_ file is. It should have a single line: `RETICULATE_PYTHON=PATH` where `PATH` is the path to your _methods3_ conda environment. Note that you must restart _R_ for the changes of creating the _.Renviron_ file to take effect- Use the commands below to find the paths:  

```{r}
library(reticulate)
print(conda_list())

```

To update your environment based on the updated `methods3_environment.yml` file, go to your _week_06_ folder and run the following from a _bash_ interpreter (e.g. _terminal_):  

```{bash, eval=FALSE}
conda env create --force -f methods3_environment.yml
```


# EXERCISE 1 - Estimate bias and variance based on a true underlying function  

We can express regression as $y = f(x) + \epsilon$ with $E[\epsilon] = 0$ and $var(\epsilon) = \sigma^2$ ($E$ means expected value)  
  
For a given point: $x_0$, we can decompose the expected prediction error , $E[(y_0 - \hat{f}(x_0))^2]$ into three parts - __bias__, __variance__ and __irreducible error__ (the first two together are the __reducible error__):

The expected prediction error is, which we also call the __Mean Squared Error__:  
$E[(y_0 - \hat{f}(x_0))^2] =  bias(\hat{f}(x_0))^2 + var(\hat{f}(x_0)) + \sigma^2$
  
where __bias__ is;
  
$bias(\hat{f}(x_0)) = E[\hat{f}(x_0)] - f(x_0)$

1) Create a function, $f(x)$ that squares its input. This is our __true__ function  
    i. generate data, $y_{true}$, based on an input range of [0, 6] with a spacing of 0.1. Call this $x$
    ii. add normally distributed noise to $y_{true}$ with $\sigma=5$ (set a seed to 7 `np.random.seed(7)`) and call it $y_{noise}$
    iii. plot the true function and the generated points 
```{python}

def truefunction(x): #a function that squares its input
    p = x**2
    return(p)


x_2 = np.arange(0, 6, 0.1) #we create input for our function

y_true = truefunction(x_2) #we generate data from our true function (is this our true function?)

num_sampl = len(y_true) #we create a vector of length of our y_true vector

np.random.seed(7) #we set the seed for our random generation of numbers below

y_noise = y_true+np.random.normal(loc=0, scale=5, size=num_sampl) # we complete the data generation with normally distributed noise added (with sigma = 5)


#we now proceed to plot the generated points above and plot the true function on top of those (CHECK AGAIN: IS THIS CORRECT???)

plt.clf() 
plt.scatter(x_2, y_noise)
plt.scatter(x_2, y_true)
#plt.plot(y_noise, 'r')#delete? #perhaps just add a regression line on top of this noise
plt.legend(['generated data (with noise)', 'true function'])
plt.show()

```
    
    
2) Fit a linear regression using `LinearRegression` from `sklearn.linear_model` based on $y_{noise}$ and $x$ (see code below)  
    i. plot the fitted line (see the `.intercept_` and `.coef_` attributes of the `regressor` object) on top of the plot (from 2.1.iii)
    ii. now run the code associated with Exercise 2.2.ii - what does X_quadratic amount to?
    iii. do a quadratic and a fifth order fit as well and plot them (on top of the plot from 2.2.i)
    
    

```{python, eval=FALSE}
# Exercise 2.2
from sklearn.linear_model import LinearRegression

num_sampl = len(y_true)
X_2 = np.zeros(shape=(num_sampl, 2))
X_2[:, 0] = x_2 ** 0 
X_2[:, 1] = x_2 ** 1 

regressor = LinearRegression().fit(X_2, y_noise)

y_hat_LG = regressor.intercept_+regressor.coef_[1]*x_2

plt.clf() 
plt.scatter(x_2, y_noise)
plt.scatter(x_2, y_true)
plt.plot(y_hat_LG)
plt.xlim([0, 6])
plt.ylim([-10, 40])
plt.show()


```

```{python, eval=FALSE}
# Exercise 2.2.ii
from sklearn.preprocessing import PolynomialFeatures
quadratic = PolynomialFeatures(degree=2)
X_quadratic = quadratic.fit_transform(x_2.reshape(-1, 1)) #This amounts to mupltiple linear regression of a polynomial equation of degree 2
regressor2 = LinearRegression().fit(X_quadratic, y_noise)

y_hat_MLG = regressor2.intercept_+regressor2.coef_[1]*x_2+regressor2.coef_[2]*x_2**2


#We now do a fifth order polynomial
num_sampl = len(y_noise)
X_5 = np.zeros(shape=(num_sampl, 3))
X_5[:, 0] = x_2 ** 0 
X_5[:, 1] = x_2 ** 1 
X_5[:, 2] = x_2**5

regressor5 = LinearRegression().fit(X_5, y_noise)

y_hat_MLG5 = regressor5.intercept_+regressor5.coef_[1]*x_2+regressor5.coef_[2]*x_2**5


#The plots
plt.figure() 
plt.scatter(x_2, y_noise) #generated data
plt.scatter(x_2, y_true) #true function
plt.plot(x_2, y_hat_LG) #Linear reg
plt.plot(x_2, y_hat_MLG) #second order lin reg
plt.plot(x_2, y_hat_MLG5)#Fifth order lin reg
plt.show()




```

3) Simulate 100 samples, each with sample size `len(x)` with $\sigma=5$ normally distributed noise added on top of the true function  
    i. do linear, quadratic and fifth-order fits for each of the 100 samples  
    
```{python}

new_samples = []
for i in range(100): # 100 new samples 
    rand_gen = np.random.normal(loc=0, scale=5, size=len(x_2))
    new_samples.append(y_true+rand_gen)

#We do linear fit for each of the 100 samples

test = len(y_true)
X_test = np.zeros(shape=(test, 2))
X_test[:, 0] = x_2 ** 0 
X_test[:, 1] = x_2 ** 1 

#simply put, we have just created 100 new samples of y_noise with the above for loop

new_yhats_100_LG = []
for i in range(100):
  regressor_100s = LinearRegression().fit(X_test, new_samples[i])
  y_hats = regressor_100s.intercept_+regressor_100s.coef_[1]*x_2
  new_yhats_100_LG.append(y_hats)

new_yhats_100_LG #This contains a linear fit for each of the 100 samples (each with a sample size of 60, i.e. len(x_2))

#We do a quadratic fit for each of the 100 samples
#This amounts to mupltiple linear regression of a polynomial equation of degree 2

test_100 = len(y_true)
X_test_100 = np.zeros(shape=(test_100, 3))
X_test_100[:, 0] = x_2 ** 0 
X_test_100[:, 1] = x_2 ** 1 
X_test_100[:, 2] = x_2 ** 2

new_yhats_100_QUAD = []
for i in range(100):
  regressor_100s_quad = LinearRegression().fit(X_test_100, new_samples[i])
  y_hats_QUAD = regressor_100s_quad.intercept_+regressor_100s_quad.coef_[1]*x_2+regressor_100s_quad.coef_[2]*x_2**2
  new_yhats_100_QUAD.append(y_hats_QUAD)
  
new_yhats_100_QUAD #This contains the y_hats for all 100 samples for the quadratic fit


#We now fit the fifth order polynomial
test_100_fifth = len(y_true)
X_test_100_fifth = np.zeros(shape=(test_100_fifth, 3))
X_test_100_fifth[:, 0] = x_2 ** 0 
X_test_100_fifth[:, 1] = x_2 ** 1 
X_test_100_fifth[:, 2] = x_2 ** 5

new_yhats_100_fifth = []
for i in range(100):
  regressor_100s_fifth = LinearRegression().fit(X_test_100_fifth, new_samples[i])
  y_hats_fifth = regressor_100s_fifth.intercept_+regressor_100s_fifth.coef_[1]*x_2+regressor_100s_fifth.coef_[2]*x_2**5
  new_yhats_100_fifth.append(y_hats_fifth)

new_yhats_100_fifth #This contains the y_hats for all 100 samples for the fifth order polynomial

```
    
    
    ii create a __new__ figure, `plt.figure`, and plot the linear and the quadratic fits (colour them appropriately); highlight the true value for $x_0=3$. From the graphics alone, judge which fit has the highest bias and which has the highest variance  
    
```{python}

plt.figure()
for i in range(0, 99):
  plt.plot(x_2, new_yhats_100_LG[i], "b-")
  plt.plot(x_2, new_yhats_100_QUAD[i], "r-")
plt.plot(3, truefunction(3), "go") #a green dot where x0=3
plt.show()


```
    
    iii. create a __new__ figure, `plt.figure`, and plot the quadratic and the fifth-order fits (colour them appropriately); highlight the true value for $x_0=3$. From the graphics alone, judge which fit has the highest bias and which has the highest variance  

```{python}

plt.figure()
for i in range(0, 99):
  plt.plot(x_2, new_yhats_100_QUAD[i], "g-")
  plt.plot(x_2, new_yhats_100_fifth[i], "b-")
plt.plot(3, truefunction(3), "yo") #a yellow dot where x0=3
plt.show()


```


    iv. estimate the __bias__ and __variance__ at $x_0$ for the linear, the quadratic and the fifth-order fits (the expected value $E[\hat{f}(x_0)]$ is found by taking the mean of all the simulated, $\hat{f}(x_0)$, differences)
    
```{python}

third_element = np.where(x == 3)[0] #the index [0] means we are looking at rows and not columns, since the np.where() function looks at matrices. In total, this means that for each array in out prediction matrices, we will look for the third index element in that particular array rowwise (and not columnwise, as we would have to specify the index by [1])

#QUESTION FOR CLASS: WHY DO WE SPECIFY "x" == 3, and not some arbitrary variable name, i.e. "z"? It doesn't seem to work if x is not specificed - is it perhaps because the elements within each array is referred to as 'x' by the array itself?

df = []
for i in new_yhats_100_LG:
  hello = i[third_element]
  df.append(hello)
df_mean = np.mean(df)
bias_LG = df_mean-truefunction(3) #This is our expected value for our linear reg predictions, i.e. in other words it is the bias value for lin reg

#Explanation: "df_mean" is equivalent to E[(y_hat)] and truefunction is equivalent to f(x) (since it is the true function we specified to begin with).

df_quad = []
for i in new_yhats_100_QUAD:
  hello_quad = i[third_element]
  df_quad.append(hello_quad)
df_mean_quad = np.mean(df_quad)
bias_QUAD = df_mean_quad-truefunction(3) #This is the bias value for quad reg

df_fifth = []
for i in new_yhats_100_fifth:
  hello_fifth = i[third_element]
  df_fifth.append(hello_fifth)
df_mean_fifth = np.mean(df_fifth)
bias_fifth = df_mean_fifth-truefunction(3) #This is the bias value for fifth order polynomial

#Here, we are able to find the bias, since we know our true function (f(x)) - since we created it ourselves at the beginning of the script. 


df_var = []
for i in new_yhats_100_LG:
  hello_var = i[third_element]
  df_var.append(hello_var)
df_var = np.var(df_var)
df_var #This is the variance for the normal Linear Regression

df_quad_var = []
for i in new_yhats_100_QUAD:
  hello_quad_var = i[third_element]
  df_quad_var.append(hello_quad_var)
df_var_quad = np.var(df_quad_var)

df_var_quad #This is the variance for the second degree polynomial

df_fifth_var = []
for i in new_yhats_100_fifth:
  hello_fifth_var = i[third_element]
  df_fifth_var.append(hello_fifth_var)
df_var_fifth = np.var(df_fifth)

df_var_fifth #This is the variance for the fifth degree polynomial


```
    
    
    v. show how the __squared bias__ and the __variance__ are related to the complexity of the fitted models  
    
```{python}

#Below, we will find the Mean Squared Error, in which the bias of the model is squared and added to the variance. The mean squared error is an indicator of model fit, where a lower MSE means a better fit - or, in other words, a model that predicts better. In the so called bias-variance trade-off, the complexity of a model increases when the bias decreases and variance increases. The bigger the value of our bias, the more we decrease the complexity of the model. The lower the value of our variance, the more the complexity of the model is also decreased. 

```
    
    vi. simulate __epsilon__: `epsilon = np.random.normal(scale=5, size=100)`. Based on your simulated values of __bias, variance and epsilon__, what is the __Mean Squared Error__ for each of the three fits? Which fit is better according to this measure?  
    
```{python}

epsilon = np.random.normal(scale = 5, size = 100)

MeanSquare_LG = df_var+bias_LG**2+np.var(epsilon)
MeanSquare_QUAD = bias_QUAD**2+df_var_quad+np.var(epsilon)
MeanSquare_fifth = bias_fifth**2+df_var_fifth+np.var(epsilon)

MeanSquare_LG
MeanSquare_QUAD
MeanSquare_fifth

#The second degree polynomial fit is the model with the lowest MSE (109.74), which indicates it is the better fit.
```
    
# EXERCISE 2: Fitting training data and applying it to test sets with and without regularization

```{python, eval=FALSE}

All references to pages are made to this book:
Raschka, S., 2015. Python Machine Learning. Packt Publishing Ltd.  

1) Import the housing dataset using the upper chunk of code from p. 280 

```{python}
import pandas as pd

house = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data', header=None, sep='\s+')

```

    i. and define the correlation matrix `cm` as done on p. 284 
```{python}

house.columns = ["CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT", "MEDV"]

house.head()
import numpy as np
cols = ["LSTAT", "INDUS", "NOX", "RM", "MEDV"]
cm = np.corrcoef(house[cols].values.T)

```
    
    ii. based on this matrix, do you expect collinearity can be an issue if we run multiple linear regression  by fitting MEDV on LSTAT, INDUS, NOX and RM?  
    
```{python}

cm

#From the correlation matrix there does seem to be a strong correlation between the variables LSTAT and INDUS (0.60), LSTAT and NOX (0.59) and LSTAT and RM (-0.74), and this might all be indicative of a collinearity issue. 

```
    
2) Fit MEDV on  LSTAT, INDUS, NOX and RM (standardize all five variables by using `StandardScaler.fit_transform`, (`from sklearn.preprocessing import StandardScaler`) by doing multiple linear regression using `LinearRegressionGD` as defined on pp. 285-286

```{python}
from sklearn.preprocessing import StandardScaler

```
```{python}

class LinearRegressionGD(object):
  def __init__(self, eta = 0.001, n_iter = 20):
    self.eta = eta
    self.n_iter = n_iter
  
  def fit(self, X, y):
    self.w_ = np.zeros(1+X.shape[1])
    self.cost_ = []
  
    for i in range(self.n_iter):
      output = self.net_input(X)
      errors = (y-output)
      self.w_[1:] += self.eta*X.T.dot(errors)
      self.w_[0] += self.eta*errors.sum()
      cost = (errors**2).sum()/2.0
      self.cost_.append(cost)
    return self
  
  def net_input(self, X):
    return np.dot(X, self.w_[1:])+self.w_[0]
  
  def predict(self, X):
    return self.net_input(X)

X = house[["LSTAT", "INDUS", "RM", "NOX"]].values
y = house["MEDV"].values

X_std = StandardScaler().fit_transform(X)
y_std = StandardScaler().fit_transform(y.reshape(-1, 1))


LinReg = LinearRegressionGD().fit(X_std, y_std) #I get an error message: "operands could not be broadcast together with shapes (4,) (4,506) (4,)" - how to fix this?


```

    i. how much does the solution improve in terms of the cost function if you go through 40 iterations instead of the default of 20 iterations? 
    
```{python}

#How do I fix the LinearRegression problem above first?

```
    
    ii. how does the residual sum of squares based on the analytic solution (Ordinary Least Squares) compare to the cost after 40 iterations?
    iii. Bonus question: how many iterations do you need before the Ordinary Least Squares and the Gradient Descent solutions result in numerically identical residual sums of squares?  
    
    
```{python}

```

3) Build your own cross-validator function. This function should randomly split the data into $k$ equally sized folds (see figure p. 176) (see the code chunk associated with exercise 2.3). It should also return the Mean Squared Error for each of the folds
```{python}

# Exercise 2.3
def cross_validate(estimator, X, y, k): # estimator is the object created by initialising LinearRegressionGD
    mses = list() # we want to return k mean squared errors
    fold_size = y.shape[0] // k # we do integer division to get a whole number of samples
    for fold in range(k): # loop through each of the folds
        begin = fold * fold_size
        end = (1+fold)*fold_size
        spectrum = np.arange(begin, end)
        X_train = ? #Mikkels comment: Had some trouble figuring out what to do here
        y_train = ? #Ibid
        X_test = ? #Ibid 
        y_test = ? #Ibid
        
        # fit training data
        # predict on test data
        # calculate MSE
        
    return mses


```

    i. Cross-validate the fits of your model from Exercise 2.2. Run 11 folds and run 500 iterations for each fit  
    
```{python}

#I can't get my first linear regression to work... 

```
    
    ii. What is the mean of the mean squared errors over all 11 folds?
    
```{python}



```
    
4) Now, we will do a Ridge Regression. Use `Ridge` (see code chunk associated with Exercise 2.4) to find the optimal `alpha` parameter ($\lambda$)
    i. Find the _MSE_ (the mean of the _MSE's_ associated with each fold) associated with a reasonable range of `alpha` values (you need to find the lambda that results in the minimum _MSE_)  
    
```{python}
# Exercise 2.4
from sklearn.linear_model import Ridge, Lasso
import numpy as np

RR = Ridge(alpha=?)
LassoR = Lasso(alpha)

def_alpha = np.arange(0.1, 100, 0.1) #We take increments of 0.1, since we want to find the minimum Mean Squared Error - I suppose taking increments of 0.1 helps us here. 
# Get mses of fitted models in range of lambdas
MeanSquareErrors = []
for i in def_alpha:
  ridgeregression = Ridge(alpha = i)
  meansquareerror = cross_validate(ridgeregression, X, y, 11) #Number 11 because we have 11 folds above ##NB Cannot complete THE LOOP BECAUSE I CANNOT GET THE cross_validate function above to work
  MeanSquareErrors.append(meansquareerror)

MeanSquareErrors

```
    
    ii. Plot the _MSE_ as a function of `alpha` ($\lambda$). Make sure to include an _MSE_ for `alpha=0` as well  
    
```{python}



```
    
    iii. Find the _MSE_ for the optimal `alpha`, compare its _MSE_ to that of the OLS regression
    
```{python}

```
    
    iv. Do the same steps for Lasso Regression `Lasso`  (2.4.i.-2.4.iii.)
    
```{python}

```
    
    v. Describe the differences between these three models, (the optimal Lasso, the optimal Ridge and the OLS)
    
```{python}

```
    


```{python, eval=FALSE}

```

```{python, eval=FALSE}
#xercise 2.4
def cross_validate(estimator, X, y, k): # estimator is the object created by initialising LinearRegressionGD
    mses = list() # we want to return k mean squared errors
    fold_size = y.shape[0] // k # we do integer division to get a whole number of samples
    for fold in range(k): # loop through each of the folds
        
        X_train = ?
        y_train = ?
        X_test = ?
        y_test = ?
        
        # fit training data
        # predict on test data
        # calculate MSE
        
    return mses


```

